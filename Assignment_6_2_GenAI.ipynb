{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1YezBGRcBAjCI1YD7jzOD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MunjaSangeetha/Generative-AI/blob/main/Assignment_6_2_GenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "M.Sangeetha\n",
        "2303A52088\n",
        "Assignmnent - 6.2\n",
        "Generative AI"
      ],
      "metadata": {
        "id": "p8LXIbVQmms3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLgZf6ValsI3",
        "outputId": "33e67141-378b-475a-e2d2-de9080687333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MSE: 153669205868.61\n",
            "Training MAE: 277348.26\n",
            "Testing MSE: 1959406221695.99\n",
            "Testing MAE: 1017470.62\n",
            "Model saved as 'housing_price_model.pkl'.\n",
            "Predicted Housing Price: 5344780.0\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/Housing.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "num_features = [\"area\", \"bedrooms\", \"bathrooms\", \"stories\", \"parking\"]\n",
        "cat_features = [\"mainroad\", \"guestroom\", \"basement\", \"hotwaterheating\", \"airconditioning\", \"prefarea\", \"furnishingstatus\"]\n",
        "\n",
        "# Define preprocessors\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), num_features),\n",
        "        (\"cat\", OneHotEncoder(drop=\"first\"), cat_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Prepare data\n",
        "X = df.drop(columns=[\"price\"])\n",
        "y = df[\"price\"]\n",
        "X_transformed = preprocessor.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Random Forest Regressor model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = rf_model.predict(X_train)\n",
        "y_test_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate error metrics\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(rf_model, \"housing_price_model.pkl\")\n",
        "\n",
        "# Print model performance\n",
        "print(f\"Training MSE: {train_mse:.2f}\")\n",
        "print(f\"Training MAE: {train_mae:.2f}\")\n",
        "print(f\"Testing MSE: {test_mse:.2f}\")\n",
        "print(f\"Testing MAE: {test_mae:.2f}\")\n",
        "\n",
        "print(\"Model saved as 'housing_price_model.pkl'.\")\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model\n",
        "rf_model = joblib.load(\"housing_price_model.pkl\")\n",
        "\n",
        "# Predict on a new sample (example input with 13 features)\n",
        "new_sample = np.array([X_test[0]])  # Use any sample from X_test\n",
        "predicted_price = rf_model.predict(new_sample)\n",
        "\n",
        "print(\"Predicted Housing Price:\", predicted_price[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import load_model\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "url = \"/content/Housing.csv\"\n",
        "dataset = pd.read_csv(url)\n",
        "\n",
        "# Assuming 'price' is the target column and the rest are features\n",
        "X = dataset.drop('price', axis=1)  # Features\n",
        "y = dataset['price']  # Target variable\n",
        "\n",
        "# ----> Identify and handle categorical columns (e.g., using OneHotEncoder) <----\n",
        "categorical_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']  # Replace with your categorical column names\n",
        "\n",
        "# Create a OneHotEncoder instance (we set handle_unknown='ignore' to avoid errors for unseen categories)\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "# Fit and transform the categorical features\n",
        "encoded_cols = encoder.fit_transform(X[categorical_cols])\n",
        "\n",
        "# Create a DataFrame from the encoded features\n",
        "encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "# Drop the original categorical columns and concatenate the encoded features\n",
        "X = X.drop(categorical_cols, axis=1)\n",
        "X = pd.concat([X, encoded_df], axis=1)\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Build the ANN model\n",
        "model = Sequential()\n",
        "\n",
        "# Hidden layers based on the provided architecture\n",
        "model.add(Dense(15, input_dim=X_train.shape[1], activation='relu'))  # Hidden Layer 1\n",
        "model.add(Dense(20, activation='relu'))  # Hidden Layer 2\n",
        "model.add(Dense(25, activation='relu'))  # Hidden Layer 3\n",
        "model.add(Dense(20, activation='relu'))  # Hidden Layer 4\n",
        "model.add(Dense(15, activation='relu'))  # Hidden Layer 5\n",
        "\n",
        "# Output layer (since we're predicting a continuous value, we use linear activation)\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Step 5: Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Step 6: Train the model\n",
        "history = model.fit(X_train, y_train, epochs=150, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Step 7: Evaluate the model\n",
        "train_loss, train_mae = model.evaluate(X_train, y_train)\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(f\"Training MAE: {train_mae}\")\n",
        "print(f\"Testing MAE: {test_mae}\")\n",
        "\n",
        "# Step 8: Save the model in .h5 format\n",
        "model.save('housing_price_predictor.h5')\n",
        "\n",
        "# Step 9: Load the model for deployment\n",
        "loaded_model = load_model('housing_price_predictor.h5')\n",
        "\n",
        "# Step 10: Make predictions with the loaded model\n",
        "predictions = loaded_model.predict(X_test)\n",
        "\n",
        "# Display predictions vs actual values for the first 10 examples\n",
        "for i in range(10):\n",
        "    print(f\"Predicted: {predictions[i][0]}, Actual: {y_test.iloc[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwaD_FdTl4D_",
        "outputId": "65845eca-ba87-4104-eff9-20371b69ced6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - loss: 23694007074816.0000 - mae: 4591082.0000 - val_loss: 30129988304896.0000 - val_mae: 5007536.0000\n",
            "Epoch 2/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 25942432940032.0000 - mae: 4761022.5000 - val_loss: 30129975721984.0000 - val_mae: 5007535.0000\n",
            "Epoch 3/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 25677982072832.0000 - mae: 4743452.5000 - val_loss: 30129954750464.0000 - val_mae: 5007532.5000\n",
            "Epoch 4/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 24932020912128.0000 - mae: 4700008.5000 - val_loss: 30129906515968.0000 - val_mae: 5007528.0000\n",
            "Epoch 5/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 26110790205440.0000 - mae: 4776190.0000 - val_loss: 30129791172608.0000 - val_mae: 5007518.0000\n",
            "Epoch 6/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24434947653632.0000 - mae: 4638595.0000 - val_loss: 30129518542848.0000 - val_mae: 5007492.0000\n",
            "Epoch 7/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 25400071684096.0000 - mae: 4708494.0000 - val_loss: 30128895688704.0000 - val_mae: 5007435.0000\n",
            "Epoch 8/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 24998286721024.0000 - mae: 4686871.5000 - val_loss: 30127570288640.0000 - val_mae: 5007317.0000\n",
            "Epoch 9/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 25879535157248.0000 - mae: 4790228.5000 - val_loss: 30124913197056.0000 - val_mae: 5007081.5000\n",
            "Epoch 10/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25422666399744.0000 - mae: 4713789.5000 - val_loss: 30119810826240.0000 - val_mae: 5006638.0000\n",
            "Epoch 11/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24197814288384.0000 - mae: 4623210.5000 - val_loss: 30110449139712.0000 - val_mae: 5005831.0000\n",
            "Epoch 12/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 25317471158272.0000 - mae: 4716547.5000 - val_loss: 30093801947136.0000 - val_mae: 5004415.0000\n",
            "Epoch 13/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 26101348827136.0000 - mae: 4767348.0000 - val_loss: 30065811259392.0000 - val_mae: 5002050.0000\n",
            "Epoch 14/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 25560285708288.0000 - mae: 4741890.0000 - val_loss: 30020456153088.0000 - val_mae: 4998239.0000\n",
            "Epoch 15/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25610246160384.0000 - mae: 4725288.0000 - val_loss: 29948895035392.0000 - val_mae: 4992264.0000\n",
            "Epoch 16/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25159773716480.0000 - mae: 4676418.5000 - val_loss: 29840830889984.0000 - val_mae: 4983262.0000\n",
            "Epoch 17/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 25052611346432.0000 - mae: 4678178.0000 - val_loss: 29680495230976.0000 - val_mae: 4969959.5000\n",
            "Epoch 18/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 24704928710656.0000 - mae: 4676889.5000 - val_loss: 29454088798208.0000 - val_mae: 4951153.5000\n",
            "Epoch 19/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25033806184448.0000 - mae: 4682844.0000 - val_loss: 29129976053760.0000 - val_mae: 4924227.0000\n",
            "Epoch 20/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 24146176114688.0000 - mae: 4616581.0000 - val_loss: 28692740833280.0000 - val_mae: 4887729.5000\n",
            "Epoch 21/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24774457688064.0000 - mae: 4621359.5000 - val_loss: 28098523299840.0000 - val_mae: 4837752.5000\n",
            "Epoch 22/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23451171553280.0000 - mae: 4557646.5000 - val_loss: 27328371490816.0000 - val_mae: 4772048.0000\n",
            "Epoch 23/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21868297846784.0000 - mae: 4402129.0000 - val_loss: 26352648454144.0000 - val_mae: 4687333.5000\n",
            "Epoch 24/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 22068835909632.0000 - mae: 4376008.0000 - val_loss: 25115819507712.0000 - val_mae: 4577287.5000\n",
            "Epoch 25/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 20290006417408.0000 - mae: 4219161.0000 - val_loss: 23641095929856.0000 - val_mae: 4441297.5000\n",
            "Epoch 26/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 17930196615168.0000 - mae: 3980700.2500 - val_loss: 21924908367872.0000 - val_mae: 4275908.0000\n",
            "Epoch 27/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 17917565468672.0000 - mae: 3940512.0000 - val_loss: 19923902398464.0000 - val_mae: 4071797.5000\n",
            "Epoch 28/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 15634105630720.0000 - mae: 3690592.5000 - val_loss: 17732904943616.0000 - val_mae: 3830616.2500\n",
            "Epoch 29/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14396581478400.0000 - mae: 3513480.5000 - val_loss: 15448553816064.0000 - val_mae: 3553548.0000\n",
            "Epoch 30/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11663783755776.0000 - mae: 3154623.5000 - val_loss: 13220900241408.0000 - val_mae: 3253099.2500\n",
            "Epoch 31/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 9437796368384.0000 - mae: 2778479.2500 - val_loss: 11141592907776.0000 - val_mae: 2951789.2500\n",
            "Epoch 32/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7981588545536.0000 - mae: 2480023.5000 - val_loss: 9312801914880.0000 - val_mae: 2673797.5000\n",
            "Epoch 33/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6390654435328.0000 - mae: 2208368.5000 - val_loss: 7889730666496.0000 - val_mae: 2425219.7500\n",
            "Epoch 34/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5974455746560.0000 - mae: 2063902.2500 - val_loss: 6820367171584.0000 - val_mae: 2212952.7500\n",
            "Epoch 35/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5094926974976.0000 - mae: 1878639.2500 - val_loss: 6109676437504.0000 - val_mae: 2055209.3750\n",
            "Epoch 36/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4679226359808.0000 - mae: 1764153.1250 - val_loss: 5596051931136.0000 - val_mae: 1937414.1250\n",
            "Epoch 37/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4467743784960.0000 - mae: 1738678.0000 - val_loss: 5208957517824.0000 - val_mae: 1854742.1250\n",
            "Epoch 38/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3836291579904.0000 - mae: 1580537.3750 - val_loss: 4912301211648.0000 - val_mae: 1794090.3750\n",
            "Epoch 39/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3670456926208.0000 - mae: 1521581.3750 - val_loss: 4686677540864.0000 - val_mae: 1746654.5000\n",
            "Epoch 40/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3654082363392.0000 - mae: 1520125.3750 - val_loss: 4476700721152.0000 - val_mae: 1704707.2500\n",
            "Epoch 41/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3609209864192.0000 - mae: 1468125.7500 - val_loss: 4288598507520.0000 - val_mae: 1667190.0000\n",
            "Epoch 42/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3584490733568.0000 - mae: 1475766.5000 - val_loss: 4128990560256.0000 - val_mae: 1636081.1250\n",
            "Epoch 43/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3075709337600.0000 - mae: 1365175.0000 - val_loss: 4028922331136.0000 - val_mae: 1616571.8750\n",
            "Epoch 44/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3064462573568.0000 - mae: 1384561.0000 - val_loss: 3910168215552.0000 - val_mae: 1594470.1250\n",
            "Epoch 45/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2670908669952.0000 - mae: 1302663.8750 - val_loss: 3813448613888.0000 - val_mae: 1575289.5000\n",
            "Epoch 46/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2918603030528.0000 - mae: 1352350.2500 - val_loss: 3744487440384.0000 - val_mae: 1560758.6250\n",
            "Epoch 47/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3136755335168.0000 - mae: 1387549.2500 - val_loss: 3660599001088.0000 - val_mae: 1542210.6250\n",
            "Epoch 48/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2493581361152.0000 - mae: 1253721.7500 - val_loss: 3586918187008.0000 - val_mae: 1525460.1250\n",
            "Epoch 49/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2534499155968.0000 - mae: 1238688.8750 - val_loss: 3525294424064.0000 - val_mae: 1510492.0000\n",
            "Epoch 50/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2689080229888.0000 - mae: 1278719.7500 - val_loss: 3470892990464.0000 - val_mae: 1496803.6250\n",
            "Epoch 51/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2589423304704.0000 - mae: 1250568.6250 - val_loss: 3436154191872.0000 - val_mae: 1487451.6250\n",
            "Epoch 52/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2505709715456.0000 - mae: 1219276.1250 - val_loss: 3384753258496.0000 - val_mae: 1473833.5000\n",
            "Epoch 53/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2164548173824.0000 - mae: 1156871.2500 - val_loss: 3347100205056.0000 - val_mae: 1463011.2500\n",
            "Epoch 54/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2010360578048.0000 - mae: 1107380.8750 - val_loss: 3313252171776.0000 - val_mae: 1453577.3750\n",
            "Epoch 55/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2361249759232.0000 - mae: 1167886.8750 - val_loss: 3301756633088.0000 - val_mae: 1449468.0000\n",
            "Epoch 56/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2163750993920.0000 - mae: 1146808.5000 - val_loss: 3254937452544.0000 - val_mae: 1437065.3750\n",
            "Epoch 57/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2477783252992.0000 - mae: 1220636.3750 - val_loss: 3217168531456.0000 - val_mae: 1427061.1250\n",
            "Epoch 58/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2046193827840.0000 - mae: 1115038.5000 - val_loss: 3193186025472.0000 - val_mae: 1421827.3750\n",
            "Epoch 59/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1986096005120.0000 - mae: 1090269.0000 - val_loss: 3173802835968.0000 - val_mae: 1417543.0000\n",
            "Epoch 60/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2278854492160.0000 - mae: 1185138.2500 - val_loss: 3160164007936.0000 - val_mae: 1414402.0000\n",
            "Epoch 61/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1843890094080.0000 - mae: 1049771.2500 - val_loss: 3128387698688.0000 - val_mae: 1407224.3750\n",
            "Epoch 62/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1944092278784.0000 - mae: 1104204.7500 - val_loss: 3107104489472.0000 - val_mae: 1402625.0000\n",
            "Epoch 63/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1961774022656.0000 - mae: 1086929.0000 - val_loss: 3089537171456.0000 - val_mae: 1398143.1250\n",
            "Epoch 64/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1966151565312.0000 - mae: 1105990.7500 - val_loss: 3055742615552.0000 - val_mae: 1391099.0000\n",
            "Epoch 65/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2043481948160.0000 - mae: 1098962.6250 - val_loss: 3029997453312.0000 - val_mae: 1385577.3750\n",
            "Epoch 66/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1852957655040.0000 - mae: 1043669.1875 - val_loss: 3017728065536.0000 - val_mae: 1381906.2500\n",
            "Epoch 67/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1612900990976.0000 - mae: 990202.0000 - val_loss: 2983758921728.0000 - val_mae: 1375922.8750\n",
            "Epoch 68/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1857715830784.0000 - mae: 1072289.7500 - val_loss: 2986903076864.0000 - val_mae: 1373983.7500\n",
            "Epoch 69/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1930039001088.0000 - mae: 1062179.3750 - val_loss: 2946396848128.0000 - val_mae: 1365947.0000\n",
            "Epoch 70/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1966642692096.0000 - mae: 1075467.5000 - val_loss: 2936076500992.0000 - val_mae: 1362122.3750\n",
            "Epoch 71/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1717073477632.0000 - mae: 1014279.8125 - val_loss: 2914129805312.0000 - val_mae: 1356808.5000\n",
            "Epoch 72/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2019668262912.0000 - mae: 1095399.7500 - val_loss: 2887943979008.0000 - val_mae: 1351447.7500\n",
            "Epoch 73/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1708609503232.0000 - mae: 1006719.3750 - val_loss: 2870341009408.0000 - val_mae: 1347299.6250\n",
            "Epoch 74/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1532315959296.0000 - mae: 973446.5000 - val_loss: 2861710704640.0000 - val_mae: 1344513.1250\n",
            "Epoch 75/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1560652414976.0000 - mae: 959132.6250 - val_loss: 2851768893440.0000 - val_mae: 1341773.2500\n",
            "Epoch 76/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1507317514240.0000 - mae: 942125.3125 - val_loss: 2824086487040.0000 - val_mae: 1335897.0000\n",
            "Epoch 77/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1601398636544.0000 - mae: 969690.1250 - val_loss: 2798070005760.0000 - val_mae: 1330654.0000\n",
            "Epoch 78/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1763423158272.0000 - mae: 1029532.6250 - val_loss: 2807112400896.0000 - val_mae: 1329375.3750\n",
            "Epoch 79/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1500724723712.0000 - mae: 955208.6250 - val_loss: 2783649726464.0000 - val_mae: 1324037.1250\n",
            "Epoch 80/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1624708612096.0000 - mae: 969696.3125 - val_loss: 2756830822400.0000 - val_mae: 1318902.8750\n",
            "Epoch 81/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1390238629888.0000 - mae: 921794.3750 - val_loss: 2740950663168.0000 - val_mae: 1315166.6250\n",
            "Epoch 82/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1612444729344.0000 - mae: 993746.9375 - val_loss: 2736272179200.0000 - val_mae: 1312826.0000\n",
            "Epoch 83/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1553854758912.0000 - mae: 951110.0625 - val_loss: 2714057310208.0000 - val_mae: 1308065.7500\n",
            "Epoch 84/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1433927286784.0000 - mae: 903912.8125 - val_loss: 2694408568832.0000 - val_mae: 1303906.2500\n",
            "Epoch 85/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1498256769024.0000 - mae: 928300.8125 - val_loss: 2677866233856.0000 - val_mae: 1299335.7500\n",
            "Epoch 86/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1530405584896.0000 - mae: 946248.5625 - val_loss: 2669660078080.0000 - val_mae: 1296264.7500\n",
            "Epoch 87/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1415655456768.0000 - mae: 900983.8125 - val_loss: 2658996060160.0000 - val_mae: 1293171.8750\n",
            "Epoch 88/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1422021230592.0000 - mae: 932945.5000 - val_loss: 2646390603776.0000 - val_mae: 1289684.3750\n",
            "Epoch 89/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1381628772352.0000 - mae: 907492.6875 - val_loss: 2634088185856.0000 - val_mae: 1286250.8750\n",
            "Epoch 90/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1558817538048.0000 - mae: 958322.8750 - val_loss: 2615489855488.0000 - val_mae: 1282856.5000\n",
            "Epoch 91/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1409810432000.0000 - mae: 896604.5000 - val_loss: 2602835640320.0000 - val_mae: 1279801.1250\n",
            "Epoch 92/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1314433269760.0000 - mae: 871641.6875 - val_loss: 2572901154816.0000 - val_mae: 1274787.5000\n",
            "Epoch 93/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1417956032512.0000 - mae: 897589.6875 - val_loss: 2581943549952.0000 - val_mae: 1273549.2500\n",
            "Epoch 94/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1453862027264.0000 - mae: 917960.1875 - val_loss: 2562845310976.0000 - val_mae: 1269336.5000\n",
            "Epoch 95/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1184388612096.0000 - mae: 825431.0625 - val_loss: 2558190944256.0000 - val_mae: 1266628.7500\n",
            "Epoch 96/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1331568967680.0000 - mae: 886984.0000 - val_loss: 2532361371648.0000 - val_mae: 1261706.6250\n",
            "Epoch 97/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1300200423424.0000 - mae: 875361.6875 - val_loss: 2531725672448.0000 - val_mae: 1259032.2500\n",
            "Epoch 98/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1266067308544.0000 - mae: 879308.6875 - val_loss: 2513522917376.0000 - val_mae: 1254874.7500\n",
            "Epoch 99/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1343767445504.0000 - mae: 874192.8125 - val_loss: 2502832160768.0000 - val_mae: 1250890.8750\n",
            "Epoch 100/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1306482704384.0000 - mae: 881972.5625 - val_loss: 2476852379648.0000 - val_mae: 1246648.7500\n",
            "Epoch 101/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1097686450176.0000 - mae: 797315.9375 - val_loss: 2472956133376.0000 - val_mae: 1243929.1250\n",
            "Epoch 102/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1253482823680.0000 - mae: 859760.6250 - val_loss: 2457509560320.0000 - val_mae: 1240169.2500\n",
            "Epoch 103/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1330121015296.0000 - mae: 881716.2500 - val_loss: 2452933574656.0000 - val_mae: 1237774.6250\n",
            "Epoch 104/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1340364816384.0000 - mae: 872007.3750 - val_loss: 2439379943424.0000 - val_mae: 1233992.7500\n",
            "Epoch 105/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1283440640000.0000 - mae: 859675.0625 - val_loss: 2410215899136.0000 - val_mae: 1229601.6250\n",
            "Epoch 106/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1287167934464.0000 - mae: 844817.6250 - val_loss: 2399892144128.0000 - val_mae: 1226108.2500\n",
            "Epoch 107/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1142036496384.0000 - mae: 816705.8750 - val_loss: 2421974892544.0000 - val_mae: 1225694.5000\n",
            "Epoch 108/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1063865286656.0000 - mae: 798656.9375 - val_loss: 2403073523712.0000 - val_mae: 1221401.7500\n",
            "Epoch 109/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1152466288640.0000 - mae: 809394.8750 - val_loss: 2380851314688.0000 - val_mae: 1216718.0000\n",
            "Epoch 110/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1260010733568.0000 - mae: 851520.2500 - val_loss: 2392946114560.0000 - val_mae: 1215750.5000\n",
            "Epoch 111/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1098469539840.0000 - mae: 804989.6250 - val_loss: 2365899407360.0000 - val_mae: 1210802.2500\n",
            "Epoch 112/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1032723300352.0000 - mae: 765008.1250 - val_loss: 2343061946368.0000 - val_mae: 1206596.0000\n",
            "Epoch 113/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1232130932736.0000 - mae: 816851.8125 - val_loss: 2341289328640.0000 - val_mae: 1204700.3750\n",
            "Epoch 114/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1225240346624.0000 - mae: 835877.3125 - val_loss: 2335118458880.0000 - val_mae: 1201954.0000\n",
            "Epoch 115/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1112242651136.0000 - mae: 779440.1875 - val_loss: 2342998769664.0000 - val_mae: 1200875.1250\n",
            "Epoch 116/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1133305004032.0000 - mae: 801187.1250 - val_loss: 2326448832512.0000 - val_mae: 1197079.2500\n",
            "Epoch 117/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1212368683008.0000 - mae: 826795.8125 - val_loss: 2310160777216.0000 - val_mae: 1193692.8750\n",
            "Epoch 118/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1135674916864.0000 - mae: 807328.0000 - val_loss: 2307479306240.0000 - val_mae: 1191220.2500\n",
            "Epoch 119/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1204368441344.0000 - mae: 809747.8750 - val_loss: 2307990487040.0000 - val_mae: 1189421.6250\n",
            "Epoch 120/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1068432949248.0000 - mae: 791090.3125 - val_loss: 2293738766336.0000 - val_mae: 1185889.6250\n",
            "Epoch 121/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1082587676672.0000 - mae: 792586.0000 - val_loss: 2275445047296.0000 - val_mae: 1182393.1250\n",
            "Epoch 122/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 987048837120.0000 - mae: 754898.8125 - val_loss: 2282883907584.0000 - val_mae: 1181326.6250\n",
            "Epoch 123/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1006470561792.0000 - mae: 769566.2500 - val_loss: 2257618468864.0000 - val_mae: 1176370.5000\n",
            "Epoch 124/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1135861039104.0000 - mae: 770477.5625 - val_loss: 2266460323840.0000 - val_mae: 1175570.7500\n",
            "Epoch 125/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1110107357184.0000 - mae: 771626.5625 - val_loss: 2252083560448.0000 - val_mae: 1172402.6250\n",
            "Epoch 126/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1023542820864.0000 - mae: 763975.8750 - val_loss: 2253367541760.0000 - val_mae: 1170786.8750\n",
            "Epoch 127/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1054985682944.0000 - mae: 772415.1875 - val_loss: 2237659873280.0000 - val_mae: 1167455.1250\n",
            "Epoch 128/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 925419175936.0000 - mae: 736195.9375 - val_loss: 2228274069504.0000 - val_mae: 1164195.2500\n",
            "Epoch 129/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1129232728064.0000 - mae: 800781.3750 - val_loss: 2229309800448.0000 - val_mae: 1163116.7500\n",
            "Epoch 130/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1031337738240.0000 - mae: 776481.2500 - val_loss: 2229252915200.0000 - val_mae: 1161106.0000\n",
            "Epoch 131/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 947674677248.0000 - mae: 737119.0625 - val_loss: 2229597110272.0000 - val_mae: 1160188.6250\n",
            "Epoch 132/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 984290492416.0000 - mae: 759993.7500 - val_loss: 2204887678976.0000 - val_mae: 1155660.7500\n",
            "Epoch 133/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 810419290112.0000 - mae: 681848.5625 - val_loss: 2202512392192.0000 - val_mae: 1154021.2500\n",
            "Epoch 134/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 945757093888.0000 - mae: 717414.7500 - val_loss: 2184277524480.0000 - val_mae: 1150332.2500\n",
            "Epoch 135/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1016391860224.0000 - mae: 767613.1250 - val_loss: 2209616232448.0000 - val_mae: 1151398.8750\n",
            "Epoch 136/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1065210150912.0000 - mae: 774110.1875 - val_loss: 2190346813440.0000 - val_mae: 1147231.0000\n",
            "Epoch 137/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1012956004352.0000 - mae: 750670.8750 - val_loss: 2181401280512.0000 - val_mae: 1144475.5000\n",
            "Epoch 138/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1026891841536.0000 - mae: 760248.5000 - val_loss: 2171731312640.0000 - val_mae: 1142079.7500\n",
            "Epoch 139/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 989490511872.0000 - mae: 767729.2500 - val_loss: 2171883618304.0000 - val_mae: 1140516.7500\n",
            "Epoch 140/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 958851776512.0000 - mae: 735884.0000 - val_loss: 2181003083776.0000 - val_mae: 1140255.8750\n",
            "Epoch 141/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 994064596992.0000 - mae: 751641.5000 - val_loss: 2154465984512.0000 - val_mae: 1134857.3750\n",
            "Epoch 142/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1016889016320.0000 - mae: 758058.7500 - val_loss: 2140605120512.0000 - val_mae: 1132150.8750\n",
            "Epoch 143/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 925467148288.0000 - mae: 734742.3125 - val_loss: 2148723589120.0000 - val_mae: 1131426.2500\n",
            "Epoch 144/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 970801545216.0000 - mae: 763918.7500 - val_loss: 2141646749696.0000 - val_mae: 1129258.7500\n",
            "Epoch 145/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 951241474048.0000 - mae: 725398.4375 - val_loss: 2140935553024.0000 - val_mae: 1127050.7500\n",
            "Epoch 146/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 930081144832.0000 - mae: 740314.2500 - val_loss: 2135139680256.0000 - val_mae: 1125393.0000\n",
            "Epoch 147/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1013217951744.0000 - mae: 757531.5625 - val_loss: 2127282307072.0000 - val_mae: 1123016.1250\n",
            "Epoch 148/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 963441590272.0000 - mae: 743561.0625 - val_loss: 2110061805568.0000 - val_mae: 1119212.7500\n",
            "Epoch 149/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1038601617408.0000 - mae: 765048.0000 - val_loss: 2104245354496.0000 - val_mae: 1117056.8750\n",
            "Epoch 150/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 971364368384.0000 - mae: 751094.5000 - val_loss: 2115034808320.0000 - val_mae: 1117138.7500\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 974393114624.0000 - mae: 739118.0625 \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1962602397696.0000 - mae: 1065057.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MAE: 736013.9375\n",
            "Testing MAE: 1117138.75\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Predicted: 3404634.75, Actual: 4060000\n",
            "Predicted: 6969035.0, Actual: 6650000\n",
            "Predicted: 3768526.0, Actual: 3710000\n",
            "Predicted: 4374420.5, Actual: 6440000\n",
            "Predicted: 3006421.25, Actual: 2800000\n",
            "Predicted: 3625738.75, Actual: 4900000\n",
            "Predicted: 5126090.5, Actual: 5250000\n",
            "Predicted: 6502882.0, Actual: 4543000\n",
            "Predicted: 1100893.375, Actual: 2450000\n",
            "Predicted: 2853128.0, Actual: 3353000\n"
          ]
        }
      ]
    }
  ]
}